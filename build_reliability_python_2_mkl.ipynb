{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T13:56:09.188166",
     "start_time": "2016-09-30T13:56:09.089645"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing required dependencies ['numpy']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8405d9f71264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda3/envs/python_2_mkl/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Missing required dependencies {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing required dependencies ['numpy']"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "class Analysis(Enum):\n",
    "    VERIFIED_RELIABLE = 'verified_reliable'\n",
    "    GUESSED_RELIABLE = 'guessed_reliable'\n",
    "    VERIFIED_CATEGORY = 'verified_category'\n",
    "\n",
    "RELIABLE, UNRELIABLE = [\"RELIABLE\", \"UNRELIABLE\"]\n",
    "\n",
    "class Load(object):\n",
    "    data_path = \"/opt/shared_venvs\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def switch_y_for_npm(X, Y):\n",
    "        for (i, (x, y)) in enumerate(zip(X, Y)):\n",
    "            if \"npm ERR\" in x:\n",
    "                Y[i] = UNRELIABLE\n",
    "\n",
    "    def get_original_idx(self, idx_in_mangled_data):\n",
    "        return self.log_path_to_idx[self.X_log_path[idx_in_mangled_data]]\n",
    "\n",
    "    def get_mangled_idx(self, original_idx):\n",
    "        return self.original_to_mangled[original_idx]\n",
    "\n",
    "    def __init__(self, analysis=Analysis.VERIFIED_RELIABLE, min_num_occurence_of_category=6):\n",
    "        categories = {}\n",
    "        curr_idx = 0\n",
    "        if analysis == Analysis.VERIFIED_RELIABLE:\n",
    "            self.category_attribute = \"verified_category\"\n",
    "            df = pd.read_csv(os.path.join(self.data_path, 'data', \"verified\",\n",
    "                                          'with_content.csv'))\n",
    "        elif analysis == Analysis.GUESSED_RELIABLE:\n",
    "            self.category_attribute = \"guessed_category\"\n",
    "            df = pd.read_csv(os.path.join(self.data_path, 'data', \"unverified\",\n",
    "                                          'with_content.csv'))\n",
    "        elif analysis == Analysis.VERIFIED_CATEGORY:\n",
    "            self.category_attribute = \"full_categories\"\n",
    "            df = pd.read_csv(os.path.join(self.data_path, 'data', \"verified\",\n",
    "                                          'with_content.csv'))\n",
    "            y = []\n",
    "            for text, (verified, category) in zip(df['text'],\n",
    "                                                  zip(df['verified_category'],\n",
    "                                                      df['categories'])):\n",
    "                new_cat = category\n",
    "                if isinstance(new_cat, float) and math.isnan(new_cat):\n",
    "                    if verified == 'RELIABLE':\n",
    "                        new_cat = verified\n",
    "                if new_cat not in categories:\n",
    "                    curr_idx += 1\n",
    "                    categories[new_cat] = (curr_idx, 0)\n",
    "                idx, val = categories[new_cat]\n",
    "                categories[new_cat] = (idx, val + 1)\n",
    "                y.append(idx)\n",
    "            new_cat = pd.Series(y, name=self.category_attribute)\n",
    "            df = pd.concat([df, new_cat], axis=1)\n",
    "        self.idx_per_category = {v[0]: k for k, v in categories.items()}\n",
    "        # filter the categories that have at least min_num_occurence_of_category\n",
    "        counts = df.groupby(self.category_attribute).aggregate(np.count_nonzero)\n",
    "        chosen_categories = counts[counts.path_to_log >= min_num_occurence_of_category].index\n",
    "        # print [\"{}: {}\".format(idx_per_category[k], categories[idx_per_category[k]][1]) for k in chosen_categories]\n",
    "        self.log_path_to_idx = {path: idx for (idx, path) in enumerate(df['path_to_log'])}\n",
    "        df = df[getattr(df, self.category_attribute).isin(chosen_categories)]\n",
    "        self.X_log_path = np.array(list(df['path_to_log']))\n",
    "        self.X_text = np.array(list(df['text']))\n",
    "        self.mangled_to_original = {i: self.get_original_idx(i)\n",
    "                                    for (i, _) in enumerate(self.X_text)}\n",
    "        self.original_to_mangled = {v: k for k, v in self.mangled_to_original.items()}\n",
    "        self.root_cause = []\n",
    "\n",
    "        try:\n",
    "            self.root_cause = [self.idx_per_category[x] for x in df['full_categories']]\n",
    "        except Exception:\n",
    "            pass\n",
    "        Y = df[self.category_attribute]\n",
    "        if analysis in (Analysis.VERIFIED_RELIABLE,):\n",
    "            self.switch_y_for_npm(self.X_text, Y)\n",
    "            unreliable = len([y for y in Y if y == \"UNRELIABLE\"])\n",
    "            reliable = len([y for y in Y if y == \"RELIABLE\"])\n",
    "            print(\"unreliable: {0}, reliable: {1}, ({2:.2f})\".format(unreliable, reliable, unreliable / float(reliable + unreliable) * 100.0))\n",
    "\n",
    "        self.Y = np.array(list(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "# python_path must be the path of python in the workers\n",
    "# i.e. if you installed libs on a virtualenv \n",
    "# available in all workers you should put\n",
    "# findspark.init(python_path=<path_virtualenv>/bin/python)\n",
    "findspark.init(python_path=\"/opt/sklearn_env/bin/python\")\n",
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"spark://tlsisbld100l:7077\")\n",
    "         .setAppName(\"testPySparkIon\")\n",
    "         .set(\"spark.executor.memory\", \"1g\")\n",
    "         #.set(\"spark.cores.max\", 100)\n",
    "         .set(\"spark.broadcast.factory\", \"org.apache.spark.broadcast.HttpBroadcastFactory\")\n",
    "         .set(\"spark.driver.port\", 7001)\n",
    "         .set(\"spark.fileserver.port\", 7002)\n",
    "         .set(\"spark.broadcast.port\", 7003)\n",
    "         .set(\"spark.replClassServer.port\", 7004)\n",
    "         .set(\"spark.blockManager.port\", 7005)\n",
    "         .set(\"spark.executor.port\", 7006))\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for parallelizing GridSearchCV jobs in scikit-learn\n",
    "\"\"\"\n",
    "\n",
    "from collections import Sized\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, is_classifier, clone\n",
    "from sklearn.cross_validation import KFold, check_cv, _fit_and_score, _safe_split\n",
    "from sklearn.grid_search import BaseSearchCV, _check_param_grid, ParameterGrid, _CVScoreTuple\n",
    "from sklearn.metrics.scorer import check_scoring\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "\n",
    "class GridSearchCV(BaseSearchCV):\n",
    "    \"\"\"Exhaustive search over specified parameter values for an estimator, using Spark to\n",
    "    distribute the computations.\n",
    "    Important members are fit, predict.\n",
    "    GridSearchCV implements a \"fit\" method and a \"predict\" method like\n",
    "    any classifier except that the parameters of the classifier\n",
    "    used to predict is optimized by cross-validation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    sc: the spark context\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        A object of that type is instantiated for each grid point.\n",
    "    param_grid : dict or list of dictionaries\n",
    "        Dictionary with parameters names (string) as keys and lists of\n",
    "        parameter settings to try as values, or a list of such\n",
    "        dictionaries, in which case the grids spanned by each dictionary\n",
    "        in the list are explored. This enables searching over any sequence\n",
    "        of parameter settings.\n",
    "    scoring : string, callable or None, optional, default: None\n",
    "        A string (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    fit_params : dict, optional\n",
    "        Parameters to pass to the fit method.\n",
    "    n_jobs : int, default 1\n",
    "        This parameter is not used and kept for compatibility.\n",
    "    pre_dispatch : int, or string, optional\n",
    "        This parameter is not used and kept for compatibility.\n",
    "    iid : boolean, default=True\n",
    "        If True, the data is assumed to be identically distributed across\n",
    "        the folds, and the loss minimized is the total loss per sample,\n",
    "        and not the mean loss across the folds.\n",
    "    cv : integer or cross-validation generator, default=3\n",
    "        A cross-validation generator to use. If int, determines\n",
    "        the number of folds in StratifiedKFold if estimator is a classifier\n",
    "        and the target y is binary or multiclass, or the number\n",
    "        of folds in KFold otherwise.\n",
    "        Specific cross-validation objects can be passed, see\n",
    "        sklearn.cross_validation module for the list of possible objects.\n",
    "    refit : boolean, default=True\n",
    "        Refit the best estimator with the entire dataset.\n",
    "        If \"False\", it is impossible to make predictions using\n",
    "        this GridSearchCV instance after fitting.\n",
    "        The refitting step, if any, happens on the local machine.\n",
    "    verbose : integer\n",
    "        Controls the verbosity: the higher, the more messages.\n",
    "    error_score : 'raise' (default) or numeric\n",
    "        Value to assign to the score if an error occurs in estimator fitting.\n",
    "        If set to 'raise', the error is raised. If a numeric value is given,\n",
    "        FitFailedWarning is raised. This parameter does not affect the refit\n",
    "        step, which will always raise the error.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn import svm, datasets\n",
    "    >>> from spark_sklearn import GridSearchCV\n",
    "    >>> from pyspark.sql import SparkSession\n",
    "    >>> from spark_sklearn.util import createLocalSparkSession\n",
    "    >>> spark = createLocalSparkSession()\n",
    "    >>> iris = datasets.load_iris()\n",
    "    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "    >>> svr = svm.SVC()\n",
    "    >>> clf = GridSearchCV(spark.sparkContext, svr, parameters)\n",
    "    >>> clf.fit(iris.data, iris.target)\n",
    "    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
    "    GridSearchCV(cv=None, error_score=...,\n",
    "           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n",
    "                         decision_function_shape=None, degree=..., gamma=...,\n",
    "                         kernel='rbf', max_iter=-1, probability=False,\n",
    "                         random_state=None, shrinking=True, tol=...,\n",
    "                         verbose=False),\n",
    "           fit_params={}, iid=..., n_jobs=1,\n",
    "           param_grid=..., pre_dispatch=..., refit=...,\n",
    "           scoring=..., verbose=...)\n",
    "    >>> spark.stop(); SparkSession._instantiatedContext = None\n",
    "    Attributes\n",
    "    ----------\n",
    "    grid_scores_ : list of named tuples\n",
    "        Contains scores for all parameter combinations in param_grid.\n",
    "        Each entry corresponds to one parameter setting.\n",
    "        Each named tuple has the attributes:\n",
    "            * ``parameters``, a dict of parameter settings\n",
    "            * ``mean_validation_score``, the mean score over the\n",
    "              cross-validation folds\n",
    "            * ``cv_validation_scores``, the list of scores for each fold\n",
    "    best_estimator_ : estimator\n",
    "        Estimator that was chosen by the search, i.e. estimator\n",
    "        which gave highest score (or smallest loss if specified)\n",
    "        on the left out data. Not available if refit=False.\n",
    "    best_score_ : float\n",
    "        Score of best_estimator on the left out data.\n",
    "    best_params_ : dict\n",
    "        Parameter setting that gave the best results on the hold out data.\n",
    "    scorer_ : function\n",
    "        Scorer function used on the held out data to choose the best\n",
    "        parameters for the model.\n",
    "    Notes\n",
    "    ------\n",
    "    The parameters selected are those that maximize the score of the left out\n",
    "    data, unless an explicit score is passed in which case it is used instead.\n",
    "    The parameters n_jobs and pre_dispatch are accepted but not used.\n",
    "    See Also\n",
    "    ---------\n",
    "    :class:`ParameterGrid`:\n",
    "        generates all the combinations of a an hyperparameter grid.\n",
    "    :func:`sklearn.cross_validation.train_test_split`:\n",
    "        utility function to split the data into a development set usable\n",
    "        for fitting a GridSearchCV instance and an evaluation set for\n",
    "        its final evaluation.\n",
    "    :func:`sklearn.metrics.make_scorer`:\n",
    "        Make a scorer from a performance metric or loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "        super(GridSearchCV, self).__init__(\n",
    "            estimator, scoring, fit_params, n_jobs, iid,\n",
    "            refit, cv, verbose, pre_dispatch, error_score)\n",
    "        self.param_grid = param_grid\n",
    "        self.grid_scores_ = None\n",
    "        _check_param_grid(param_grid)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Run fit with all sets of parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
    "            Target relative to X for classification or regression;\n",
    "            None for unsupervised learning.\n",
    "        \"\"\"\n",
    "        return self._fit(X, y, ParameterGrid(self.param_grid))\n",
    "\n",
    "    def _fit(self, X, y, parameter_iterable):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "        self.scorer_ = check_scoring(self.estimator, scoring=self.scoring)\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "        cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if isinstance(parameter_iterable, Sized):\n",
    "                n_candidates = len(parameter_iterable)\n",
    "                print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                      \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                         n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        param_grid = [(parameters, train, test)\n",
    "                      for parameters in parameter_iterable\n",
    "                      for (train, test) in cv]\n",
    "        # Because the original python code expects a certain order for the elements, we need to\n",
    "        # respect it.\n",
    "        indexed_param_grid = list(zip(range(len(param_grid)), param_grid))\n",
    "        par_param_grid = sc.parallelize(indexed_param_grid, len(indexed_param_grid))\n",
    "        X_bc = sc.broadcast(X)\n",
    "        y_bc = sc.broadcast(y)\n",
    "\n",
    "        scorer = self.scorer_\n",
    "        verbose = self.verbose\n",
    "        fit_params = self.fit_params\n",
    "        error_score = self.error_score\n",
    "        fas = _fit_and_score\n",
    "\n",
    "        def fun(tup):\n",
    "            (index, (parameters, train, test)) = tup\n",
    "            local_estimator = clone(base_estimator)\n",
    "            local_X = X_bc.value\n",
    "            local_y = y_bc.value\n",
    "            res = fas(local_estimator, local_X, local_y, scorer, train, test, verbose,\n",
    "                                  parameters, fit_params,\n",
    "                                  return_parameters=True, error_score=error_score)\n",
    "            return (index, res)\n",
    "        indexed_out0 = dict(par_param_grid.map(fun).collect())\n",
    "        out = [indexed_out0[idx] for idx in range(len(param_grid))]\n",
    "\n",
    "        X_bc.unpersist()\n",
    "        y_bc.unpersist()\n",
    "\n",
    "        # Out is a list of triplet: score, estimator, n_test_samples\n",
    "        n_fits = len(out)\n",
    "        n_folds = len(cv)\n",
    "\n",
    "        scores = list()\n",
    "        grid_scores = list()\n",
    "        for grid_start in range(0, n_fits, n_folds):\n",
    "            n_test_samples = 0\n",
    "            score = 0\n",
    "            all_scores = []\n",
    "            for this_score, this_n_test_samples, _, parameters in \\\n",
    "                    out[grid_start:grid_start + n_folds]:\n",
    "                all_scores.append(this_score)\n",
    "                if self.iid:\n",
    "                    this_score *= this_n_test_samples\n",
    "                    n_test_samples += this_n_test_samples\n",
    "                score += this_score\n",
    "            if self.iid:\n",
    "                score /= float(n_test_samples)\n",
    "            else:\n",
    "                score /= float(n_folds)\n",
    "            scores.append((score, parameters))\n",
    "            # TODO: shall we also store the test_fold_sizes?\n",
    "            grid_scores.append(_CVScoreTuple(\n",
    "                parameters,\n",
    "                score,\n",
    "                np.array(all_scores)))\n",
    "        # Store the computed scores\n",
    "        self.grid_scores_ = grid_scores\n",
    "\n",
    "        # Find the best parameters by comparing on the mean validation score:\n",
    "        # note that `sorted` is deterministic in the way it breaks ties\n",
    "        best = sorted(grid_scores, key=lambda x: x.mean_validation_score,\n",
    "                      reverse=True)[0]\n",
    "        self.best_params_ = best.parameters\n",
    "        self.best_score_ = best.mean_validation_score\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                **best.parameters)\n",
    "            if y is not None:\n",
    "                best_estimator.fit(X, y, **self.fit_params)\n",
    "            else:\n",
    "                best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "class NotSparseTfidfTransformer(TfidfTransformer):\n",
    "    # XGBoost and preprocessing/standard scaler don't\n",
    "    # work with sparse data\n",
    "    # def fit_transform(self, X, Y=None):\n",
    "    #     r = super(NotSparseTfidfTransformer, self).fit_transform(X)\n",
    "    #     return r.toarray()\n",
    "\n",
    "    def transform(self, X):\n",
    "        r = super(NotSparseTfidfTransformer, self).transform(X)\n",
    "        return r.toarray()\n",
    "\n",
    "def run(analysis, shuffle=True):\n",
    "    l = Load(analysis=analysis, min_num_occurence_of_category=10)\n",
    "    cv_param = StratifiedKFold(l.Y, n_folds=3, shuffle=shuffle)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(min_df=1,\n",
    "                                 # token_pattern=r'\\b\\w+\\b'\n",
    "                                 token_pattern=r'\\b[a-zA-Z_]+\\b')),\n",
    "        ('tfidf', NotSparseTfidfTransformer()),\n",
    "        ('chi2', SelectKBest(chi2)),\n",
    "        ('scaler', preprocessing.StandardScaler()),\n",
    "        # ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "        #                       n_iter=100000,\n",
    "        #                       random_state=random.randint(0, 1000))),\n",
    "        ('clf', RandomForestClassifier())\n",
    "        # ('clf', XGBClassifier())\n",
    "    ])\n",
    "    param_grid = {'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "                  'vect__max_df': (0.5, 0.75, 1.0),\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'tfidf__norm': ('l1', 'l2'),\n",
    "                  'chi2__k': (100, 200, 500),\n",
    "                  #\n",
    "                  'scaler__with_mean': (True, False),\n",
    "                  'clf__n_estimators': (100, 200, 500,)}\n",
    "    #param_grid = {'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #              'vect__max_df': (0.5,),\n",
    "    #              'tfidf__use_idf': (True,),\n",
    "    #              'tfidf__norm': ('l1',),\n",
    "                  # 'chi2__k': (100, 200, 500),\n",
    "                  #\n",
    "    #              'scaler__with_mean': (True,),\n",
    "    #              'clf__n_estimators': (100,)}\n",
    "    clf = GridSearchCV(pipeline,\n",
    "                       param_grid=param_grid,\n",
    "                       cv=3,\n",
    "                       n_jobs=-1)\n",
    "    res = cross_val_score(clf,\n",
    "                          l.X_text,\n",
    "                          cv=cv_param,\n",
    "                          y=l.Y)\n",
    "    return sum(res) / len(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_n(analysis, shuffle, n):\n",
    "    return [run(analysis, shuffle=shuffle) for _ in range(n)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "categories = [Analysis.VERIFIED_CATEGORY, Analysis.VERIFIED_RELIABLE, Analysis.GUESSED_RELIABLE]\n",
    "to_browse = sum([[(x, False), (x, True)] for x in categories], [])\n",
    "for analysis, shuffle in to_browse:\n",
    "    res[\"cat={}, shuffle={}\".format(analysis, shuffle)] = run_n(analysis, shuffle, 30)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def dump(res):\n",
    "    buggy_res_path = '/opt/shared_venvs/results/buggy_res.json'\n",
    "    with open(buggy_res_path, 'w') as f:\n",
    "        json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python_2_mkl]",
   "language": "python",
   "name": "conda-env-python_2_mkl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
