{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T15:37:22.416292",
     "start_time": "2016-10-07T15:37:22.406519"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T15:37:22.899254",
     "start_time": "2016-10-07T15:37:22.893909"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use \"python_2\" conda environment, matching \"Python 2\" kernel. We need to use the same Python version in this kernel, which will act as a \"Spark Driver\", than the environment used in the Spark Executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T15:37:23.329708",
     "start_time": "2016-10-07T15:37:23.325734"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert sys.version_info.major < 3, (\"Python 3 not supported on our Spark environment, \"\n",
    "                                    \"please use this notebook in Python 2.7 kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bosch Production Line Kaggle Challenge\n",
    "\n",
    "This notebook analyze the data provided in the [Bosch Challenge](https://www.kaggle.com/c/bosch-production-line-performance). 2016.\n",
    "\n",
    "This challenge provides \"one of the largest datasets (in terms of number of features) ever hosted on Kaggle\", that makes is a very good candidate of \"Big Data\" investigation using our fresh new Spark Infrastructure.\n",
    "\n",
    "Beware of the various enviromnents:\n",
    "- This Notebook hosted on a intranet server\n",
    "- It uses a Kernel is also running on this machine. This kernel is a Python 2 kernel, because Python 2 is installed on the Spark Cluster and we need to use the same Python version. It uses an anaconda installation with some useful libraries.\n",
    "- The Spark Driver run on the same kernel, so also on the same machine\n",
    "- The Spark Executors are remote machine, and will use another Python installation.\n",
    "\n",
    "Some deps are installed on the current env. They are Python dependencies to be installed on the Jupyter environment, ie, they may **not** be available on the Spark Cluster Python environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T15:37:24.139433",
     "start_time": "2016-10-07T15:37:24.126024"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = os.environ['PYSPARK_SUBMIT_ARGS'].replace(\"--total-executor-cores 10\", \"--total-executor-cores 20\")\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = os.environ['PYSPARK_SUBMIT_ARGS'].replace(\"--executor-memory 16G\", \"--executor-memory 64G\")\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']\n",
    "\n",
    "def injectPackageInSpark(package_name, package_string):\n",
    "    if package_name not in os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\"):\n",
    "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\" --packages {} \".format(package_string) + os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\"))\n",
    "\n",
    "def initPySpark():\n",
    "    if \"pyspark-shell\" not in os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\"):\n",
    "        # If defined, PYSPARK_SUBMIT_ARGS needs to specify the shell to use\n",
    "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] += \" pyspark-shell\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to connect to our Spark Cluster. The environment variables are **not** set yet so PySpark can be found and connect to the cluster.\n",
    "\n",
    "We use findspark that can inject the right environment variables to initialize the `pyspark` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-07T15:37:26.602136",
     "start_time": "2016-10-07T15:37:26.598152"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(python_path=\"/opt/sklearn_env/bin/python\")  # This python_path points to the Python to use on the Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T14:48:02.325801",
     "start_time": "2016-10-14T14:48:02.256494"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'injectPackageInSpark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbb5ad5d2579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minjectPackageInSpark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark-csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'com.databricks:spark-csv_2.10:1.5.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minjectPackageInSpark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sparkxgboost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rotationsymmetry:sparkxgboost:0.2.1-s_2.10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minitPySpark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's start a nice Spark Context to help us dealing with large dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'injectPackageInSpark' is not defined"
     ]
    }
   ],
   "source": [
    "injectPackageInSpark('spark-csv', 'com.databricks:spark-csv_2.10:1.5.0')\n",
    "injectPackageInSpark('sparkxgboost', 'rotationsymmetry:sparkxgboost:0.2.1-s_2.10')\n",
    "initPySpark()\n",
    "\n",
    "print(\"Let's start a nice Spark Context to help us dealing with large dataset\")\n",
    "\n",
    "import pyspark\n",
    "spark_master = os.environ['SPARK_MASTER']\n",
    "conf = (pyspark.SparkConf()\n",
    "            .setMaster(\"spark://\" + spark_master + \":7077\")\n",
    "            .setAppName(\"bosch-challenge\")\n",
    "            .set(\"spark.executor.memory\", \"16g\")\n",
    "            .set(\"spark.cores.max\", 100)\n",
    "            .set(\"spark.broadcast.factory\", \"org.apache.spark.broadcast.HttpBroadcastFactory\")\n",
    "            .set(\"spark.driver.port\", 7001)\n",
    "            .set(\"spark.fileserver.port\", 7002)\n",
    "            .set(\"spark.broadcast.port\", 7003)\n",
    "            .set(\"spark.replClassServer.port\", 7004)\n",
    "            .set(\"spark.blockManager.port\", 7005)\n",
    "            .set(\"spark.executor.port\", 7006))\n",
    "try:\n",
    "    # Do not use the \"master\" argument here since it will ignore the --total-executor-cores argument\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except ValueError:\n",
    "    # Ignore reinitialization errors\n",
    "    pass\n",
    "print(\"Spark version: \" + sc.version)\n",
    "print(\"Spark Application name: \" + sc.appName)\n",
    "\n",
    "assert \"local[\" not in sc.master, \"PySpark running in local mode!\"\n",
    "print(\"PySpark is configured against the cluster: {}\".format(sc.master))\n",
    "print(\"Spark UI can be found at: http://\" + spark_master + \":8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:55:21.143940",
     "start_time": "2016-09-30T16:55:21.109483"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import humanize\n",
    "def getFileSize(filepath):\n",
    "    statinfo = os.stat(filepath)\n",
    "    return humanize.naturalsize(statinfo.st_size)\n",
    "print(\"Competition files on HDFS:\\n\" + \n",
    "      \"\\n\".join(\"/sbrouil/bosch/{}: {}\".format(f, getFileSize(os.path.join(\"/mnt/hdfs/sbrouil/bosch\", f))) for f in os.listdir(\"/mnt/hdfs/sbrouil/bosch\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is pretty big, the complete training set is > 6Gb, and loading this on a single computer is really slow and will make pandas suffering too much. Let's use Spark to distribute this analysis accross serveral computer. It can also be used to distribute independent computation using the sklearn environment running on each \"executor\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-20T13:11:31.811085",
     "start_time": "2016-09-20T13:11:31.804525"
    }
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "Data comes in CSV file stored on HDFS. Let's try to load them into a dataframe. If it is really slow, we will transform it into Parquet file to see the difference.\n",
    "\n",
    "Let's load the data. We will use the data inference to extract numerical values as integer, but keep in mind this will require the Spark job to process ALL data. I don't know how to provide a schema, since the number of column depends on the number of features and may be different from one file to another (and this is the case, like we will see in a few cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:55:23.995877",
     "start_time": "2016-09-30T16:55:23.987649"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext = pyspark.SQLContext(sc)\n",
    "\n",
    "# Spark is connected to the Hadoop cluster so it can access to the CSV file directly\n",
    "def get_df_from_csv(csvFile, hasHeader=False, customSchema=None, inferschema='true'):\n",
    "    df = (sqlContext\n",
    "            .read\n",
    "            .format('com.databricks.spark.csv')\n",
    "            .options(header=hasHeader, inferschema=inferschema))\n",
    "    if customSchema:\n",
    "        df = df.schema(customSchema)\n",
    "    return df.load(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:55:40.685473",
     "start_time": "2016-09-30T16:55:25.277252"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_categorical = get_df_from_csv(\"/sbrouil/bosch/train_categorical.csv\", hasHeader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:55:50.594125",
     "start_time": "2016-09-30T16:55:40.688206"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_date = get_df_from_csv(\"/sbrouil/bosch/train_date.csv\", hasHeader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:01.776981",
     "start_time": "2016-09-30T16:55:50.596581"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric = get_df_from_csv(\"/sbrouil/bosch/train_numeric.csv\", hasHeader=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some 20s' for loading all the data!\n",
    "\n",
    "Please note that Spark has found the structure of these column alone, and thanks the Scala implementation (nothing here happens on Python), it has parsed each column pretty faster to find the write data type (integer, string...).\n",
    "\n",
    "It is so convinient to be able to swimg along the **entier** data set, transparently using several dozen of machines with plenty of memory in it. We can work seamlessly in Python by drive a distributed computation is a faster language (Scala/JVM). Doing it only with Pandas, so on our local machine, involve loading all data in memory and hoping for our numpy installation to use some parallelism. Parallel computing is not so easy to do in Python, but here it is really easy to do **and so fast**!\n",
    "\n",
    "You can click on the following link to see the application run on the Spark cluster. Click on \"Application Detail UI\" and then on \"Event Timeline\" to get nice visualization of what the cluster is doing with the job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:01.784332",
     "start_time": "2016-09-30T16:56:01.779380"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"http://\" + spark_master + \":8080/app/?appId=\" + sc.applicationId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now discover our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:01.793178",
     "start_time": "2016-09-30T16:56:01.786732"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractStationInfo(name):\n",
    "    line, station, feature = name.split(\"_\")\n",
    "    return line[1:], station[1:], feature[1:]\n",
    "\n",
    "def fmtStationInfo(name):\n",
    "    line, station, feature = extractStationInfo(name)\n",
    "    return \"line {}, station {}, and feature number {}\".format(line, station, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how each training set table looks like.\n",
    "\n",
    "- The `train_date` should hold dates.\n",
    "- The `train_categorical` should hold categories.\n",
    "- The `train_numeric` should hold numeric values and is told to hold the \"Response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:02.428283",
     "start_time": "2016-09-30T16:56:02.347972"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_numeric.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T17:03:32.043133",
     "start_time": "2016-09-30T17:03:31.949440"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_categorical.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T17:03:32.327462",
     "start_time": "2016-09-30T17:03:32.243818"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_date.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is an \"Id\" in each table, and in `train_numeric`, there is the `Response` column we need to predict in the test set.\n",
    "\n",
    "How many features appart of Id and Response?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T17:06:57.679542",
     "start_time": "2016-09-30T17:06:56.121520"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first_row_train_numeric = train_numeric.drop(\"Id\").drop(\"Response\").head(1)[0]\n",
    "first_row_train_categorical = train_categorical.drop(\"Id\").head(1)[0]\n",
    "first_row_train_date = train_date.drop(\"Id\").head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T17:07:47.441615",
     "start_time": "2016-09-30T17:07:47.432747"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_numeric = len(first_row_train_numeric)\n",
    "nb_categorical = len(first_row_train_categorical)\n",
    "nb_date = len(first_row_train_date)\n",
    "\n",
    "print(\"Number of columns in 'train_numeric': {}\".format(nb_numeric))\n",
    "print(\"Number of columns in 'train_categorical': {}\".format(nb_categorical))\n",
    "print(\"Number of columns in 'train_date': {}\".format(nb_date))\n",
    "print(\"Total number of features (out of Id and Response): {}\".format(nb_numeric + nb_categorical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's look at the first rows of the 'date' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:03.791897",
     "start_time": "2016-09-30T16:56:02.606581"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "train_date.select(\"*\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T17:39:50.894851",
     "start_time": "2016-09-22T17:39:50.884745"
    }
   },
   "source": [
    "There are lot of features (`Lx_Sx_Dx`), and lot of missing information (`Nan`). Note that all the columns are NOT displayed on this Pandas table.\n",
    "\n",
    "What about 'numeric' dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:04.598026",
     "start_time": "2016-09-30T16:56:03.793861"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_numeric.select(\"*\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are some `None` and `NaN` values. [According to this page](https://www.kaggle.com/c/bosch-production-line-performance/forums/t/22909/expeditive-exploration-models-on-data), there could be a reason behind this.\n",
    "\n",
    "Let's look at categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:05.841519",
     "start_time": "2016-09-30T16:56:04.600625"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_categorical.select(\"*\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-22T17:40:06.995672",
     "start_time": "2016-09-22T17:40:06.987539"
    }
   },
   "source": [
    "Weird, there is only empty data? Let's take a random column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:20.998478",
     "start_time": "2016-09-30T16:56:05.843940"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_categorical.select(\"L0_S1_F31\").where(isnan(\"L0_S1_F31\")).limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing.\n",
    "\n",
    "First, we see that the features does not exactly match. First example on `date` we have `L0_S0_D1` while on `numeric` we have `L0_S0_F0` and in `categorical` we have `L0_S1_F25`.\n",
    "\n",
    "According to the data description, the features are not so easy to deal with. Let's first reorganize features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T15:18:18.295046",
     "start_time": "2016-09-30T15:18:17.929351"
    },
    "collapsed": false
   },
   "source": [
    "# Features Organization\n",
    "\n",
    "Along these 3 training tables, feature are related but does not 1-1 match. \n",
    "\n",
    "Let look at the [data description](https://www.kaggle.com/c/bosch-production-line-performance/data).\n",
    "\n",
    "> The dataset contains an extremely large number of anonymized features. Features are named according to a convention that tells you the production line, the station on the line, and a feature number. E.g. L3_S36_F3939 is a feature measured on line 3, station 36, and is feature number 3939.\n",
    "\n",
    "> On account of the large size of the dataset, we have separated the files by the type of feature they contain: numerical, categorical, and finally, a file with date features. The date features provide a timestamp for when each measurement was taken. Each date column ends in a number that corresponds to the previous feature number. E.g. the value of L0_S0_D1 is the time at which L0_S0_F0 was taken.\n",
    "\n",
    "Features are referenced by column label. Date timestamp is indicated by the `LX_SX_DXX` that follows the `LX_SX_FXX` feature For example, numerical feature `L0_S1_F24` and categorical feature `L0_S1_F25` and is likely timestamped by `L0_S1_D26`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:56:21.034092",
     "start_time": "2016-09-30T16:56:21.000992"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def organize(features):\n",
    "    line_features = {}\n",
    "    station_features = {}\n",
    "    lines = set([f.split('_')[0] for f in features])\n",
    "    stations = set([f.split('_')[1] for f in features])\n",
    "    \n",
    "    for l in lines:\n",
    "        line_features[l] = [f for f in features if l+'_' in f]\n",
    "        \n",
    "    for s in stations:\n",
    "        station_features[s] = [f for f in features if s+'_' in f]\n",
    "        \n",
    "            \n",
    "    return line_features, station_features\n",
    "\n",
    "line_features, station_features = orgainize(features)\n",
    "\n",
    "print(\"Features in Station 32: {}\".format( station_features['S32'] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How, we have an 'Id' column that looks like and idea, let's join all tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:45:19.929308",
     "start_time": "2016-09-30T14:44:09.811Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_list  = sorted([cell for cell in first_row_train_categorical[1:]])\n",
    "print(\"Station list for 'train_categorical':\\n\" + \"\\n\".join(\"{}: {}\".format(c, fmtStationInfo(c)) for c in station_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:45:19.929686",
     "start_time": "2016-09-30T14:44:09.813Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_list  = sorted(set([cell for cell in first_row_train_date][1:]]))\n",
    "print(\"Station list for 'train_date':\\n\" + \"\\n\".join(str(c) for c in station_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:45:19.930055",
     "start_time": "2016-09-30T14:44:09.815Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_list  = sorted(set([cell for cell in first_row_train_numeric[1:]]))\n",
    "print(\"Station list for 'train_numeric':\\n\" + \"\\n\".join(str(c) for c in station_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:45:19.930432",
     "start_time": "2016-09-30T14:44:09.816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_categorical.write.parquet(\"/gsemet/bosch/train_categorical.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:45:19.930792",
     "start_time": "2016-09-30T14:44:09.818Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_categorical = sc.read.parquet(\"/gsemet/bosch/train_categorical.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-30T16:45:19.931150",
     "start_time": "2016-09-30T14:44:09.820Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_categorical.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python_2_openblas]",
   "language": "python",
   "name": "conda-env-python_2_openblas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "notify_time": "10"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
